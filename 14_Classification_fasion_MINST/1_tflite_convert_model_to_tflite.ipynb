{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of tflite_c03_exercise_convert_model_to_tflite.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za8-Nr5k11fh"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Eq10uEbw0E4l"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUrRaN4w3ct"
      },
      "source": [
        "# Train Your Own Model and Convert It to TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3UojxdNw8J1"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c03_exercise_convert_model_to_tflite.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c03_exercise_convert_model_to_tflite.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXX-pi1r6NfG"
      },
      "source": [
        "This notebook uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>\n",
        "\n",
        "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) datasetâ€”often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc.) in a format identical to that of the articles of clothing we'll use here.\n",
        "\n",
        "This uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n",
        "\n",
        "We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjOAfhgd__Sp"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfyZKowNAQ4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8338f6e-c892-45f0-bc58-6c80d4cadc32"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCVXuXk-Ghdp"
      },
      "source": [
        "import os"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CPgnBCIIFP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67d10dcc-1e00-4524-b049-fec8c3e5b3c0"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tadPBTEiAprt"
      },
      "source": [
        "# Download Fashion MNIST Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds9gfZKzAnkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ffc076a-4a0e-4d3b-b54a-e615530a3811"
      },
      "source": [
        "splits, info = tfds.load('fashion_mnist', with_info=True, as_supervised=True, \n",
        "                         split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'])\n",
        "\n",
        "(train_examples, validation_examples, test_examples) = splits\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset fashion_mnist/3.0.1 (download: 29.45 MiB, generated: 36.42 MiB, total: 65.87 MiB) to /root/tensorflow_datasets/fashion_mnist/3.0.1...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/fashion_mnist/3.0.1.incompleteMJ173H/fashion_mnist-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/fashion_mnist/3.0.1.incompleteMJ173H/fashion_mnist-test.tfrecord\n",
            "\u001b[1mDataset fashion_mnist downloaded and prepared to /root/tensorflow_datasets/fashion_mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eAv71FRm4JE"
      },
      "source": [
        "class_names = ['T-shirt_top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXe6jNokqX3_"
      },
      "source": [
        "with open('labels.txt', 'w') as f:\n",
        "  f.write('\\n'.join(class_names))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0RxpwTmQN-y"
      },
      "source": [
        "IMG_SIZE = 28"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAkuq0V0Aw2X"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5SIivkunKCC"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQMIkJf9AvJ4"
      },
      "source": [
        "# Write a function to normalize and resize the images\n",
        "\n",
        "def format_example(image, label):\n",
        "  # Cast image to float32\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  # Resize the image if necessary\n",
        "  image = tf.reshape(image, [IMG_SIZE,IMG_SIZE,1])\n",
        "  # image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "  # Normalize the image in the range [0, 1]\n",
        "  image = image/255.0\n",
        "  return image, label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEQP743aMv4C"
      },
      "source": [
        "# Set the batch size to 32\n",
        "\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM4HfIJtnNEk"
      },
      "source": [
        "## Create a Dataset from images and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOL4gSUARFjM"
      },
      "source": [
        "# Prepare the examples by preprocessing the them and then batching them (and optionally prefetching them)\n",
        "\n",
        "# If you wish you can shuffle train set here\n",
        "train_batches = train_examples.map(format_example).shuffle(1000).batch(BATCH_SIZE)\n",
        "validation_batches = validation_examples.map(format_example).shuffle(1000).batch(BATCH_SIZE)\n",
        "test_batches = test_examples.map(format_example).shuffle(1000).batch(1)\n",
        "\n",
        "# solution:\n",
        "# train_batches = train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "# validation_batches = validation_examples.cache().batch(BATCH_SIZE).map(format_example).prefetch(1)\n",
        "# test_batches = test_examples.cache().batch(1).map(format_example)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzbUph1XQ1Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2671fc-6337-4e0d-a5bd-c8361673061d"
      },
      "source": [
        "train_examples"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-topQaOm_LM"
      },
      "source": [
        "# Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gsYqdIlEFVg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "798b32ef-6db5-4b7e-c407-563e49e438bc"
      },
      "source": [
        "\"\"\"\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
        "_________________________________________________________________\n",
        "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
        "_________________________________________________________________\n",
        "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 3872)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 64)                247872    \n",
        "_________________________________________________________________\n",
        "dense_1 (Dense)              (None, 10)                650       \n",
        "=================================================================\n",
        "Total params: 253,322\n",
        "Trainable params: 253,322\n",
        "Non-trainable params: 0\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nModel: \"sequential\"\\n_________________________________________________________________\\nLayer (type)                 Output Shape              Param #   \\n=================================================================\\nconv2d (Conv2D)              (None, 26, 26, 16)        160       \\n_________________________________________________________________\\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \\n_________________________________________________________________\\nconv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \\n_________________________________________________________________\\nflatten (Flatten)            (None, 3872)              0         \\n_________________________________________________________________\\ndense (Dense)                (None, 64)                247872    \\n_________________________________________________________________\\ndense_1 (Dense)              (None, 10)                650       \\n=================================================================\\nTotal params: 253,322\\nTrainable params: 253,322\\nNon-trainable params: 0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDqcwksFB1bh"
      },
      "source": [
        "# Build the model shown in the previous cell\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  # Set the input shape to (28, 28, 1), kernel size=3, filters=16 and use ReLU activation,  \n",
        "  tf.keras.layers.Conv2D(16,3,activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,1)),    \n",
        "  tf.keras.layers.MaxPooling2D(),\n",
        "  # Set the number of filters to 32, kernel size to 3 and use ReLU activation \n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  tf.keras.layers.Flatten(),\n",
        "  # Add a fully connected layer with 64 hidden units and ReLU activation\n",
        "  tf.keras.layers.Dense(64, activation='relu'),\n",
        "  # Attach a final softmax classification head\n",
        "  tf.keras.layers.Dense(10)])\n",
        "\n",
        "# Set the loss and accuracy metrics\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "      "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Lp3c-7Rv_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95316955-7ad1-4ea7-a3b5-a0ee03e45763"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3872)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                247872    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 253,322\n",
            "Trainable params: 253,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEMOz-LDnxgD"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGlNoRtzCP4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1425840b-f25f-4f58-c8ee-984d9272b414"
      },
      "source": [
        "model.fit(train_batches, \n",
        "          epochs=10,\n",
        "          validation_data=validation_batches)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 9s 6ms/step - loss: 0.4644 - accuracy: 0.8336 - val_loss: 0.3309 - val_accuracy: 0.8782\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3067 - accuracy: 0.8894 - val_loss: 0.2900 - val_accuracy: 0.8908\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2594 - accuracy: 0.9044 - val_loss: 0.2515 - val_accuracy: 0.9085\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2250 - accuracy: 0.9166 - val_loss: 0.2449 - val_accuracy: 0.9142\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1954 - accuracy: 0.9269 - val_loss: 0.2396 - val_accuracy: 0.9138\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1706 - accuracy: 0.9365 - val_loss: 0.2538 - val_accuracy: 0.9135\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1461 - accuracy: 0.9464 - val_loss: 0.2695 - val_accuracy: 0.9120\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1268 - accuracy: 0.9532 - val_loss: 0.2688 - val_accuracy: 0.9170\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1114 - accuracy: 0.9591 - val_loss: 0.2887 - val_accuracy: 0.9190\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0967 - accuracy: 0.9647 - val_loss: 0.2972 - val_accuracy: 0.9157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0252fe9898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZT9-7w9n4YO"
      },
      "source": [
        "# Exporting to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dq78KBkCV2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b8f646-413e-47fe-fe80-c3b33ccb5040"
      },
      "source": [
        "export_dir = 'saved_model/1'\n",
        "\n",
        "# Use the tf.saved_model API to export the SavedModel\n",
        "tf.saved_model.save(model, export_dir)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDGiYrBdE6fl"
      },
      "source": [
        "#@title Select mode of optimization\n",
        "mode = \"Speed\" #@param [\"Default\", \"Storage\", \"Speed\"]\n",
        "\n",
        "if mode == 'Storage':\n",
        "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "  optimization = tf.lite.Optimize.DEFAULT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbcS9C00CzGe"
      },
      "source": [
        "# Use the TFLiteConverter SavedModel API to initialize the converter\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=export_dir)\n",
        "# Set the optimzations\n",
        "# converter.optimizations = [optimization]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5PWCDsTC3El"
      },
      "source": [
        "tflite_model_file = 'model.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR6wFcQ1Fglm"
      },
      "source": [
        "# Test if your model is working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3IFOcUEIzQx"
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors index\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKcToCBEC-Bu"
      },
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "for img, label in test_batches.take(50):\n",
        "  interpreter.set_tensor(input_index, img)\n",
        "  interpreter.invoke()\n",
        "  predictions.append(interpreter.get_tensor(output_index))\n",
        "  test_labels.append(label[0])\n",
        "  test_images.append(np.array(img))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "kSjTmi05Tyod"
      },
      "source": [
        "#@title Utility functions for plotting\n",
        "# Utilities for plotting\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  img = np.squeeze(img)\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label.numpy():\n",
        "    color = 'green'\n",
        "  else:\n",
        "    color = 'red'\n",
        "    \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(list(range(10)), class_names, rotation='vertical')\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array[0], color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array[0])\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('green')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZZwg0wFaVXhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "94f04d94-98a5-4c3d-919c-76c94cd5e47f"
      },
      "source": [
        "#@title Visualize the outputs { run: \"auto\" }\n",
        "index = 24 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_images)\n",
        "plt.show()\n",
        "plot_value_array(index, predictions, test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076bo3FMpRDb"
      },
      "source": [
        "# Download TFLite model and assets\n",
        "\n",
        "**NOTE: You might have to run to the cell below twice**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsPXqPlgZPjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "12c2d57e-2b93-4bb9-a663-12ee69ab7ab2"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download(tflite_model_file)\n",
        "  files.download('labels.txt')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1dea1536-70a7-426b-824a-86d9803f87bf\", \"model.tflite\", 259552)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8d9240c5-169b-44cb-8a5d-5b11d1d5bb1f\", \"labels.txt\", 75)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyBVNwAzH3Oe"
      },
      "source": [
        "# Deploying TFLite model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdfa5L6wH87u"
      },
      "source": [
        "Now once you've the trained TFLite model downloaded, you can ahead and deploy this on an Android/iOS application by placing the model assets in the appropriate location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLY6X8P90L0P"
      },
      "source": [
        "# Prepare the test images for download (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3bjzLj10OJv"
      },
      "source": [
        "!mkdir -p test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVrBZv1-0Py-"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "for index, (image, label) in enumerate(test_batches.take(50)):\n",
        "  image = tf.cast(image * 255.0, tf.uint8)\n",
        "  image = tf.squeeze(image).numpy()\n",
        "  pil_image = Image.fromarray(image)\n",
        "  pil_image.save('test_images/{}_{}.jpg'.format(class_names[label[0]].lower(), index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX0N0M8u0R2s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae198d8-77ec-4b5f-c7a6-1ca2a1903244"
      },
      "source": [
        "!ls test_images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'ankle boot_24.jpg'   bag_42.jpg     dress_18.jpg      shirt_41.jpg\n",
            "'ankle boot_32.jpg'   bag_44.jpg     dress_6.jpg       shirt_48.jpg\n",
            "'ankle boot_37.jpg'   coat_11.jpg    pullover_0.jpg    shirt_4.jpg\n",
            "'ankle boot_43.jpg'   coat_15.jpg    pullover_1.jpg    sneaker_12.jpg\n",
            "'ankle boot_7.jpg'    coat_17.jpg    pullover_20.jpg   trouser_22.jpg\n",
            "'ankle boot_8.jpg'    coat_21.jpg    pullover_33.jpg   trouser_27.jpg\n",
            " bag_10.jpg\t      coat_31.jpg    pullover_38.jpg   trouser_35.jpg\n",
            " bag_16.jpg\t      coat_34.jpg    pullover_5.jpg    trouser_39.jpg\n",
            " bag_25.jpg\t      coat_40.jpg    sandal_14.jpg     t-shirt_top_23.jpg\n",
            " bag_28.jpg\t      coat_46.jpg    sandal_26.jpg     t-shirt_top_2.jpg\n",
            " bag_29.jpg\t      coat_49.jpg    sandal_45.jpg     t-shirt_top_3.jpg\n",
            " bag_30.jpg\t      coat_9.jpg     sandal_47.jpg\n",
            " bag_36.jpg\t      dress_13.jpg   shirt_19.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvLht1QM0W8k"
      },
      "source": [
        "!zip -qq fmnist_test_images.zip -r test_images/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdOq-4sT0X95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "21eb1d6e-2abd-42db-a25c-1124d4218f0b"
      },
      "source": [
        "try:\n",
        "  files.download('fmnist_test_images.zip')\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a14f4b89-f20a-4c09-8190-c2d82e37f43f\", \"fmnist_test_images.zip\", 40089)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}